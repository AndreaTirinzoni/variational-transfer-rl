\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{thmtools, thm-restate}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\mm}{\operatornamewithlimits{mm}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\title{Formatting instructions for NIPS 2018}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) on both the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

\section{Introduction}

\section{Background}

\subsection{Markov Decision Processes}

We define a Markov decision process (MDP) as a tuple $\mathcal{M} = \langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},p_0,\gamma\rangle$, where $\mathcal{S}$ is the state-space, $\mathcal{A}$ is a finite set of actions, $\mathcal{P}(\cdot | s,a)$ is the distribution of the next state $s'$ given that action $a$ is taken in state $s$, $\mathcal{R}: \mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is the reward function, $p_0$ is the initial-state distribution, and $\gamma\in [0,1)$ is the discount factor. We assume the reward function to be uniformly bounded by a constant $R_{max}>0$. A deterministic policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ is a mapping from states to actions. At the beginning of each episode of interaction, the initial state $s_0$ is drawn from $p_0$. Then, the agent takes the action $a_0 = \pi(s_0)$, receives a reward $\mathcal{R}(s_0,a_0)$, transitions to the next state $s_1 \sim \mathcal{P}(\cdot | s_0,a_0)$, and the process is repeated. The goal is to find the policy maximizing the long-term return over a possibly infinite horizon: $\max_{\pi}J(\pi)\triangleq\mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid \mathcal{M},\pi]$. To this end, we define the optimal value function $Q^*(s,a)$ as the expected return obtained by taking action $a$ in state $s$ and following an optimal policy thereafter. Then, an optimal policy $\pi^*$ is a policy that is greedy with respect to the optimal value function, i.e., $\pi^*(s) = \argmax_a Q^*(s,a)$ for all states $s$. It can be shown (e.g., \cite{puterman1994markov}) that $Q^*$ is the unique fixed-point of the optimal Bellman operator $T$ defined by $TQ(s,a) = \mathcal{R}(s,a) + \gamma\mathbb{E}_{\mathcal{P}}[\max_{a'}Q(s',a')]$ for any value function $Q$. From now on, we adopt the term $Q$-function to denote any plausible value function, i.e., any function $Q : \mathcal{S}\times\mathcal{A} \rightarrow \mathbb{R}$ uniformly bounded by $\frac{R_{max}}{1-\gamma}$.

When learning the optimal value function, a quantity of interest is how close a given $Q$-function is to the fixed-point of the Bellman operator. This is given by its Bellman residual, defined by $B(Q) \triangleq TQ - Q$. Notice that $Q$ is optimal if, and only if, $B(Q)(s,a) = 0$ for all $s,a$. Furthermore, if we assume the existence of a distribution $\mu$ over $\mathcal{S}\times\mathcal{A}$, the expected squared Bellman error of $Q$ is defined as the expected squared Bellman residual of $Q$ under $\mu$, $\mathbb{E}_{\mu}\left[ B^2(Q) \right]$. Although minimizing the empirical Bellman error is an appealing objective, it is well-known that an unbiased estimator requires two independent samples of the next state $s'$ of each $s,a$ (e.g., \cite{} \todo{cite Maillard}). In practice, the empirical Bellman error is typically replaced by the TD error, which approximates the former using a single transition sample. Given a dataset of $N$ samples, the TD error is computed as $\frac{1}{N}\sum_{i=1}^N (r_i + \gamma \max_{a'} Q(s_i',a') - Q(s_i,a_i))^2$.

\subsection{Variational Inference}

When working with Bayesian approaches, the posterior distribution of hidden variables $\bm{w} \in \mathbb{R}^K$ given data $D$,
\begin{equation}\label{eq:bayes}
p(\bm{w} | D) = \frac{p(D | \bm{w})p(\bm{w})}{p(D)} = \frac{p(D | \bm{w})p(\bm{w})}{\int_{\bm{w}} p(D | \bm{w})p(\bm{w})},
\end{equation}
is typically intractable for many models of interest (e.g., when working with deep neural networks) due to difficulties in computing the integral of Eq. \eqref{eq:bayes}. The main intuition behind variational inference \cite{} \todo{CITE} is to approximate the intractable posterior $p(\bm{w} | D)$ with a simpler distribution $q_{\bm{\xi}}(\bm{w})$. The latter is chosen in a parametric family, with variational parameters $\bm{\xi}$, as the minimizer of the Kullback-Leibler (KL) divergence w.r.t. $p$:
\begin{equation}
\min_{\bm{\xi}} KL\left(q_{\bm{\xi}}(\bm{w})\ ||\ p(\bm{w} \mid D)\right)
\end{equation}
It is well-known that minimizing the KL divergence is equivalent to maximizing the so-called \textit{evidence lower bound} (ELBO), which is defined as:
\begin{equation}
\text{ELBO}(\bm{\xi}) = \mathbb{E}_{\bm{w} \sim q_{\bm{\xi}}}\left[ \log p(D | \bm{w}) \right] - KL\left(q_{\bm{\xi}}(\bm{w})\ ||\ p(\bm{w})\right)
\end{equation}
Intuitively, the best approximation is the one that maximizes the expected log-likelihood of the data, while minimizing the KL divergenge w.r.t. the prior $p(\bm{w})$.

\section{Variational Transfer Learning}



\subsection{Algorithm}

\subsection{Gaussian Variational Transfer}

\subsection{Mixture of Gaussian Variational Transfer}

\section{Theoretical Analysis}

In this section, we theoretically analyze our variational transfer algorithm...

A first important question that we need to answer is whether replacing max with mellow-max in the Bellman operator constitutes a strong approximation or not. It has been proved \cite{} \todo{Cite MM} that the mellow Bellman operator is a contraction under the $L_{\infty}$-norm and, thus, has a unique fixed-point.  However, how such fixed-point differs from the one of the optimal Bellman operator remains an open question. Since mellow-max monotonically converges to max as $\kappa \rightarrow \infty$, it would be desirable if the corresponding operator also monotonically converged to the optimal one. We confirm that this property actually holds in the following theorem.

\begin{restatable}{theorem}{thmm} \label{th:mm}
Let $V$ be the fixed-point of the optimal Bellman operator $T$, and $Q$ the corresponding action-value function. Define the action-gap function $g(s)$ as the difference between the value of the best action and the second best action at each state $s$. Let $\wt{V}$ be the fixed-point of the mellow Bellman operator $\wt{T}$ with parameter $\kappa > 0$ and denote by $\beta > 0$ the inverse temperature of the induced Boltzmann distribution (as in \cite{}\todo{Cite MM}). Let $\nu$ be a probability measure over the state-space and $p \geq 1$. Then:
\begin{equation}
\norm{V-\wt{V}}_{\nu,p}^p \leq\ \frac{2R_{max}}{1-\gamma}\norm{1 - \frac{1}{1 + \abs{\mathcal{A}}e^{-\beta g}}}_{\nu,p}^p
\end{equation}
\end{restatable}

\section{Related Works}

\section{Experiments}

\subsection{Gridworld}

\subsection{Classic Control}

\subsection{Maze Navigation}

\section{Conclusion}

{\small 
\bibliography{biblio.bib}
\bibliographystyle{plain}
}

\newpage
\appendix

\section{Proofs of Theorems}

\thmm*
\begin{proof}
We begin by noticing that:
\begin{align*}
\norm{V-\wt{V}}_{\nu,p}^p =\ & \norm{TV - \wt{T}\wt{V}}_{\nu,p}^p \\ =\ &
\norm{TV - \wt{T}V + \wt{T}V - \wt{T}\wt{V}}_{\nu,p}^p \\ \leq\ &
\norm{TV - \wt{T}V}_{\nu,p}^p + \norm{\wt{T}V - \wt{T}\wt{V}}_{\nu,p}^p \\ \leq\ &
\norm{TV - \wt{T}V}_{\nu,p}^p + \gamma\norm{V - \wt{V}}_{\nu,p}^p
\end{align*}
where the first inequality follows from Minkowsky's inequality and the second one from the contraction property of the mellow Bellman operator. This implies that:
\begin{align} \label{eq:mm1}
\norm{V-\wt{V}}_{\nu,p}^p \leq \frac{1}{1-\gamma}\norm{TV - \wt{T}V}_{\nu,p}^p
\end{align}
Let us bound the norm on the right-hand side separately. In order to do that, we will bound the function $\abs{TV(s) - \wt{T}V(s)}$ point-wisely for any state $s$. By applying the definition of the optimal and mellow Bellman operators, we obtain:
\begin{align*}
\abs{TV(s) - \wt{T}V(s)} =\ & \abs{\max_{a}\lbrace R(s,a) + \gamma\mathbb{E}\left[V(s')\right] \rbrace - \mm_{a}\lbrace R(s,a) + \gamma\mathbb{E}\left[V(s')\right] \rbrace}\\ =\ & \abs{\max_a Q(s,a) - \mm_a Q(s,a)}
\end{align*}
Recall that applying the mellow-max is equivalent to computing an expectation under a Boltzmann distribution with inverse temperature $\beta$ induced by $\kappa$ \cite{}\todo{Cite MM}. Thus, we can write:
\begin{align} \label{eq:mm2}
\abs{\max_a Q(s,a) - \mm_a Q(s,a)} =\ & \abs{\sum_a \pi^*(a|s)Q(s,a) - \sum_a \pi_{\beta}(a|s)Q(s,a)}\notag\\ =\ & \abs{\sum_a Q(s,a) \left( \pi^*(a|s) - \pi_{\beta}(a|s)  \right)}\notag\\ \leq\ & \sum_a \abs{Q(s,a)} \abs{\pi^*(a|s) - \pi_{\beta}(a|s)}\notag\\ \leq\ & \frac{R_{max}}{1-\gamma}\sum_a\abs{\pi^*(a|s) - \pi_{\beta}(a|s)}
\end{align}
where $\pi^*$ is the optimal (deterministic) policy w.r.t. $Q$ and $\pi_{\beta}$ is the Boltzmann distribution induced by $Q$ with inverse temperature $\beta$:
\begin{align*}
\pi_{\beta}(a|s) = \frac{e^{\beta Q(s,a)}}{\sum_{a'}e^{\beta Q(s,a')}}
\end{align*}
Denote by $a_1(s)$ the optimal action for state $s$ under $Q$. We can then write:
\begin{align} \label{eq:mm3}
\sum_a\abs{\pi^*(a|s) - \pi_{\beta}(a|s)} =\ & \abs{\pi^*(a_1(s)|s) - \pi_{\beta}(a_1(s)|s)} + \sum_{a \neq a_1(s)}\abs{\pi^*(a|s) - \pi_{\beta}(a|s)}\notag\\ =\ & \abs{1 - \pi_{\beta}(a_1(s)|s)} + \sum_{a \neq a_1(s)}\abs{\pi_{\beta}(a|s)}\notag\\ =\ & 2\abs{1 - \pi_{\beta}(a_1(s)|s)}
\end{align}
Finally, let us bound this last term:
\begin{align} \label{eq:mm4}
\abs{1 - \pi_{\beta}(a_1(s)|s)} =\ & \abs{1 - \frac{e^{\beta Q(s,a_1(s))}}{\sum_{a'}e^{\beta Q(s,a')}}}\notag\\ =\ & \abs{1 - \frac{e^{\beta \left( Q(s,a_1(s)) - Q(s,a_2(s)) \right)}}{\sum_{a'}e^{\beta \left( Q(s,a') - Q(s,a_2(s))\right)}}}\notag\\ =\ & \abs{1 - \frac{e^{\beta g(s)}}{\sum_{a'}e^{\beta \left( Q(s,a') - Q(s,a_2(s))\right)}}}\notag\\ =\ & \abs{1 - \frac{e^{\beta g(s)}}{e^{\beta g(s)} + \sum_{a' \neq a_1(s)}e^{\beta \left( Q(s,a') - Q(s,a_2(s))\right)}}}\notag\\ \leq & \abs{1 - \frac{e^{\beta g(s)}}{e^{\beta g(s)} + \abs{\mathcal{A}}}}\notag\\ = & \abs{1 - \frac{1}{1 + \abs{\mathcal{A}}e^{-\beta g(s)}}}
\end{align}
Combining Eq. \eqref{eq:mm2}, \eqref{eq:mm3}, and \eqref{eq:mm4}, we obtain:
\begin{align*}
\abs{\max_a Q(s,a) - \mm_a Q(s,a)} \leq\ \frac{2R_{max}}{1-\gamma}\abs{1 - \frac{1}{1 + \abs{\mathcal{A}}e^{-\beta g(s)}}}
\end{align*}
Taking the norm and plugging this into Eq. \eqref{eq:mm1} concludes the proof.
\end{proof}

\end{document}