\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{todonotes}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Active Transfer Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Preliminaries}

\paragraph{Markov Decision Processes}
We define a Markov decision process (MDP) as a tuple $\mathcal{M} = \langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},p_0,\gamma\rangle$, where $\mathcal{S}$ is the state-space, $\mathcal{A}$ is a finite set of actions, $\mathcal{P}(\cdot | s,a)$ is the distribution of the next state $s'$ given that action $a$ is taken in state $s$, $\mathcal{R}: \mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is the reward function, $p_0$ is the initial-state distribution, and $\gamma\in [0,1)$ is the discount factor. We assume the reward function to be uniformly bounded by a constant $R_{max}>0$. A deterministic policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ is a mapping from states to actions. At the beginning of each episode of interaction, the initial state $s_0$ is drawn from $p_0$. Then, the agent takes the action $a_0 = \pi(s_0)$, receives a reward $\mathcal{R}(s_0,a_0)$, transitions to the next state $s_1 \sim \mathcal{P}(\cdot | s_0,a_0)$, and the process is repeated. The goal is to find the policy maximizing the long-term return over a possibly infinite horizon: $\max_{\pi}J(\pi)=\mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid \mathcal{M},\pi]$. To this end, we define the optimal value function $Q^*(s,a)$ as the expected return obtained by taking action $a$ in state $s$ and following an optimal policy thereafter. Then, an optimal policy $\pi^*$ is a policy that is greedy with respect to the optimal value function, i.e., $\pi^*(s) = \argmax_a Q^*(s,a)$ for all states $s$. It can be shown (e.g., \cite{puterman1994markov}) that $Q^*$ is the unique fixed-point of the optimal Bellman operator $T$ defined by $TQ(s,a) = \mathcal{R}(s,a) + \gamma\mathbb{E}_{\mathcal{P}}[\max_{a'}Q(s',a')]$ for any value function $Q$. From now on, we adopt the term $Q$-function to denote any plausible value function, i.e., any function $Q : \mathcal{S}\times\mathcal{A} \rightarrow \mathbb{R}$ uniformly bounded by $\frac{R_{max}}{1-\gamma}$.

We define the Bellman residual of a $Q$-function $Q$ as $B(Q) \triangleq TQ - Q$. Notice that a $Q$-function $Q$ is optimal if, and only if, $B(Q)(s,a) = 0$ for all $s,a$.

\paragraph{Multitask Settings}

We represent tasks $\tau$ as MDPs with shared state and action spaces, but with potentially different values for all other parameters. We assume the existence of a distribution $\mathcal{D}$ over tasks, i.e., $\tau \sim \mathcal{D}$, and we suppose that we are able to sample from such distribution.

\section{Approach}

We start by noticing that the task distribution $\mathcal{D}$ clearly induces a distribution over optimal $Q$-functions. Then, our goal is to estimate such distribution from the set of source tasks and use it as a prior for speeding up the learning process in the target task.

Consider solving the target task given a dataset $D$ of $N$ samples. The posterior distribution over optimal $Q$-functions is:
\begin{equation}
P(Q \mid D) \propto P(D \mid Q)P(Q),
\end{equation}
where $P(D \mid Q)$ is the likelihood of observing the dataset $D$ given that $Q$ is optimal and $P(Q)$ is the prior distribution induced by $\mathcal{D}$. Computing the likelihood requires knowing the transition and reward models of the current task (i.e., the MDP model), which are not available in practice. However, since we are conditioned on the fact that $Q$ is optimal, we can derive a simple approximation. We consider the empirical Bellman error $||B(Q)||_{p,D}^p$ under the $l_p$-norm defined as:
\begin{equation}
||B(Q)||_{p,D}^p = \frac{1}{N}\sum_{i=1}^N \left\lvert r_i + \gamma\max_{a'}Q(s_i',a') - Q(s_i,a_i) \right\rvert^p
\end{equation}
Assuming states and actions to be drawn from a fixed joint distribution $\mu$, we have $||B(Q)||_{p,\mu}^p = 0$ whenever $Q$ is optimal. Then, the probability of observing a dataset $D$ clearly decreases exponentially as $||B(Q)||_{p,D}^p$, for  an optimal $Q$, increases. This can be seen, for instance, by applying Hoeffding's inequality. Thus, a natural way to model the likelihood $P(D \mid Q)$ is:
\begin{equation}
P(D \mid Q) \propto e^{-\lambda N ||B(Q)||_{p,D}^p}
\end{equation}
where $\lambda$ is a constant hyperparameter. Intuitively, this indicates that $D$ is more likely when it induces low empirical Bellman error under an optimal $Q$-function. Furthermore, as the number of available samples $N$ increases, the distribution becomes more peaked at zero since the empirical Bellman error converges to the true Bellman error. In the limit $N \rightarrow \infty$:
\begin{equation}
P(D \mid Q) \propto I_{\left\{ ||B(Q)||_{p,D}^p = 0 \right\}}
\end{equation}

\subsection{Regularized Bellman Residual Minimization with Gaussian Priors}

Taking the maximum of the log-posterior, we obtain the following optimization problem:
\begin{equation}
\min_{Q\in\mathcal{Q}} ||B(Q)||_{p,D}^p - \log P(Q) 
\end{equation}
Let us specify a particular hypothesis space $\mathcal{Q}$. We consider $Q$-functions $Q_{\bm{w}}$ parameterized by the vector $\bm{w}$. Our optimization problem becomes:
\begin{equation}
\min_{\bm{w}} ||B(\bm{w})||_{p,D}^p - \log P(\bm{w}) 
\end{equation}
We model the prior distribution over the optimal parameters $\bm{w}$ as a Gaussian. That is, we assume:
\begin{equation}
P(\bm{w}) = \mathcal{N}(\bm{\mu},\bm{\Sigma})
\end{equation}
Then, our optimization, adopting the $l_2$-norm, becomes:
\begin{equation} \label{eq:optim}
\min_{\bm{w}} ||B(\bm{w})||_{D}^2 + ||\bm{w}-\bm{\mu}||_{\bm{\Sigma}} = \min_{\bm{w}} \frac{1}{N}\sum_{i=1}^N\left\lvert y_i - Q_{\bm{w}}(s_i,a_i) \right\rvert^2 + ||\bm{w}-\bm{\mu}||_{\bm{\Sigma}}
\end{equation}
where $y_i = r_i + \gamma\max_{a'}Q_{\bm{w}}(s_i',a')$ and $||\bm{x}||_{\bm{A}} \triangleq \bm{x}^T\bm{A}^{-1}\bm{x}$ for $\bm{A}$ positive definite matrix.

\paragraph{Linear model}

We assume a linear model for the $Q$-functions: $Q_{\bm{w}}(s,a) = \bm{w}^T\bm{\phi}(s,a)$. Here $\bm{\phi}$ is a $K$-dimensional feature vector. Then, the solution to the optimization problem of Eq. \eqref{eq:optim} can be computed in closed form as follows:
\begin{equation}
\bm{w}^* = \left( \bm{A}^T\bm{A} + \bm{\Sigma}^{-1} \right)^{-1} \left( \bm{A}^T\bm{b} + \bm{\Sigma}^{-1}\bm{\mu} \right) 
\end{equation}
where $\bm{A}$ is an $N\times K$ matrix containing the feature vectors at each data point $(s_i,a_i)$ and $\bm{b}$ is an $N$-dimensional vector containing their targets $y_i$. The resulting algorithm is a regularized version of LSVI.

\paragraph{Neural network}

We model $Q_{\bm{w}}(s,a)$ as a neural network with parameters $\bm{w}$. Then, the gradient of the objective function of Eq. \eqref{eq:optim} can be easily computed by standard backpropagation. The resulting algorithm is a regularized version of NFQI.

\subsection{Variational Inference for Efficient Exploration}

A major drawback of the maximum-a-posteriori (MAP) approach is that it does not explicitly estimate the posterior distribution $P(Q \mid D)$. Thus, given a dataset $D$, the algorithm consistently chooses the same MAP $Q$-function, and the only way to allow learning is to introduce a simple form of exploration (e.g., $\epsilon$-greedy or softmax), whose drawbacks are well-known. The advantages of a more time-coherent exploration strategy such as posterior sampling (or Thompson sampling) have already been proved by many existing works (CITE). In particular, direct posterior sampling of $Q$-functions (also known as value function randomization), when a distribution is available, have been proven to be very effective and scalable. Thus, we known extend our previous approach to explicitly estimate $P(Q \mid D)$, so as to allow efficient exploration of the target task. The main complication is that, as mentioned earlier, we cannot compute the likelihood $P(D \mid Q)$ explicitly, but we only have access to a model that is proportional to it. Then, estimating the posterior distribution is clearly intractable since it requires computing the marginal likelihood $P(D) = \int P(D \mid Q) dQ$. Thus, we resort variational inference (CITE) to approximate the intractable posterior with a simpler distribution from which we are able to sample.

The main idea behind variational inference is to approximate the posterior $P(Q | D)$ with a simpler distribution $q_{\bm{\phi}}(Q)$, parameterized by the vector $\bm{\phi}$, from which we can easily get samples. The best approximation is chosen in terms of the Kullback-Leibler (KL) divergence between the two distributions, that is:
\begin{equation}
\min_{\bm{\phi}} KL\left(q_{\bm{\phi}}(Q)\ ||\ P(Q \mid D)\right)
\end{equation}
It is well-known that minimizing the KL divergence is equivalent to maximizing the so-called \textit{evidence lower bound} (ELBO), which is defined as:
\begin{equation}
\text{ELBO}(\bm{\phi}) = \mathbb{E}_{Q \sim q_{\bm{\phi}}}\left[ \log P(D | Q) \right] - KL\left(q_{\bm{\phi}}(Q)\ ||\ P(Q)\right)
\end{equation}
As before, we assume a parametric form $Q_{\bm{w}}$ for representing $Q$-functions. Thus, we reduce distributions over functions to distributions over vectors $\bm{w} \in \mathbb{R}^K$. Given a specific form for the approximate posterior $q_{\bm{\phi}(\bm{w})}$ and the prior $P(\bm{w})$, our inference problem reduces to:
\begin{equation} \label{eq:vi}
\min_{\bm{\phi}} \mathbb{E}_{\bm{w} \sim q_{\bm{\phi}}}\left[ \lambda N ||B(\bm{w})||_{D}^2 \right] + KL\left(q_{\bm{\phi}}(\bm{w})\ ||\ P(\bm{w})\right)
\end{equation}
Intuitively, the approximate posterior trades-off between placing probability mass over those weights $\bm{w}$ that have low empirical Bellman error (first term), and staying close to the prior distribution that is learned from the set of source tasks (second term). The factor $N$ multiplying $||B(\bm{w})||_{D}^2$ makes sure that, as the number of samples goes to infinity, only the first term is considered in the optimization. In such case, all probability mass go over the (approximate) fixed-point of the optimal Bellman operator. On the other hand, for small values of $N$, there is less evidence that the empirical Bellman error closely estimate the true one, thus giving more importance to staying close to the prior.

Depending on the choice of the prior and approximate posterior distributions, the optimization problem of Eq. \eqref{eq:vi} might still be very hard to solve. We now propose some suitable choices that make the problem efficiently solvable.

\paragraph*{Gaussian distributions}

We model both the prior and the approximate posterior as Gaussian distributions: $P(\bm{w}) = \mathcal{N}(\bar{\bm{\mu}},\bar{\bm{\Sigma}})$ and $q_{\bm{\phi}}(\bm{w}) = \mathcal{N}(\bm{\mu},\bm{\Sigma})$, respectively. For the sake of simplicity, we assume $\bm{\Sigma} = \text{diag}(\sigma_1^2,\dots,\sigma_K^2)$ and $\bar{\bm{\Sigma}} = \text{diag}(\bar{\sigma}_1^2,\dots,\bar{\sigma}_K^2)$. Notice, in this case, that the parameter vector $\bm{\phi}$ is $[\mu_1,\dots,\mu_K,\sigma_1^2,\dots,\sigma_K^2]$. The KL divergence between the two distributions can be computed in closed form as:
\begin{equation}
KL\left(q_{\bm{\phi}}(\bm{w})\ ||\ P(\bm{w})\right) = \frac{1}{2}\left( \log\frac{\left\lvert \bar{\bm{\Sigma}} \right\rvert}{\left\lvert \bm{\Sigma} \right\rvert} + \text{Tr}\left( \bar{\bm{\Sigma}}^{-1}\bm{\Sigma} \right) + (\bm{\mu} - \bar{\bm{\mu}})^T \bar{\bm{\Sigma}}^{-1} (\bm{\mu} - \bar{\bm{\mu}}) -K \right)
\end{equation}
To solve the optimization problem of Eq. \eqref{eq:vi} we adopt stochastic backpropagation (CITE). Thus, we need to compute the gradient of the objective function $\mathcal{L}(\bm{\phi})$ of Eq. \eqref{eq:vi} with respect to each component of $\bm{\phi}$. This can be easily done for the second term:
\begin{equation}
\nabla_{\bm{\mu}} KL\left(q_{\bm{\phi}}(\bm{w})\ ||\ P(\bm{w})\right) = \bar{\bm{\Sigma}}^{-1} (\bm{\mu} - \bar{\bm{\mu}})
\end{equation}
\begin{equation}
\nabla_{\sigma_k^2} KL\left(q_{\bm{\phi}}(\bm{w})\ ||\ P(\bm{w})\right) = \frac{1}{2}\left( \frac{1}{\bar{\sigma}_k^2} - \frac{1}{\sigma_k^2} \right)
\end{equation}
For the first term, we have to compute the gradient of an expectation with respect to the parameters of the Gaussian distribution under which the expectation is taken. For the mean, we can resort to Thm. X of (CITE Bonnet 1964):
\begin{equation}
\nabla_{\mu_k} \mathbb{E}_{\bm{w} \sim \mathcal{N}(\bm{\mu},\bm{\Sigma})}\left[ \lambda N ||B(\bm{w})||_{D}^2 \right] = \mathbb{E}_{\bm{w} \sim \mathcal{N}(\bm{\mu},\bm{\Sigma})}\left[ \lambda N \nabla_{w_k} ||B(\bm{w})||_{D}^2 \right]
\end{equation}
For the covariance, we can resort to Thm. Y of (CITE Price 1958):
\begin{equation}
\nabla_{\sigma_k^2} \mathbb{E}_{\bm{w} \sim \mathcal{N}(\bm{\mu},\bm{\Sigma})}\left[ \lambda N ||B(\bm{w})||_{D}^2 \right] = \frac{1}{2}\mathbb{E}_{\bm{w} \sim \mathcal{N}(\bm{\mu},\bm{\Sigma})}\left[ \lambda N \nabla_{w_k,w_k}^2 ||B(\bm{w})||_{D}^2 \right]
\end{equation}
We refer the reader to (CITE Deepmind) for a simple proofs of these two theorems.

It only remains to compute the gradient and Hessian of the empirical Bellman error $||B(\bm{w})||_{D}^2$ with respect to $\bm{w}$. Recall that:
\begin{equation}
||B(\bm{w})||_{D}^2 = \frac{1}{N}\sum_{i=1}^N \left( r_i + \gamma\max_{a'}Q_{\bm{w}}(s_i',a') - Q_{\bm{w}}(s_i,a_i) \right)^2
\end{equation}



\section{Related Works}

List of any paper that relates to our approach together with a brief description:
\begin{itemize}
\item \cite{osband2014generalization}: the authors propose a method for efficient exploration via randomized value functions. The optimal $Q$-function is computed by bayesian LSVI and, after each update, parameters are sampled from the posterior. Then, the agent follows a greedy policy with respect to the sampled $Q$-function. Regret bounds are provided.
\item \cite{osband2016deep}: the authors extend the idea of randomized value functions to drive exploration in deep RL. A posterior distribution over $Q$-functions is approximated via bootstrapping. In each episode, the agent acts greedily with respect to a $Q$-function sampled from the approximated posterior.
\item \cite{chen2017ucb}: the authors build on top of bootstrapped DQNs to provide UCB-like exploration bonuses. In a previous (?) version of the paper, exploration bonuses based on information gain are also proposed.
\end{itemize}

\section{Experiments}

\section{Conclusion}

{\small 
\bibliography{biblio.bib}
\bibliographystyle{plain}
}
\newpage
\appendix

\section{Proofs}

\end{document}
