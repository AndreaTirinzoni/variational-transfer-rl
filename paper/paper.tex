\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{todonotes}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Active Transfer Learning}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Preliminaries}

\paragraph{Markov Decision Processes}
We define a Markov decision process (MDP) as a tuple $\mathcal{M} = \langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},p_0,\gamma\rangle$, where $\mathcal{S}$ is the state-space, $\mathcal{A}$ is a finite set of actions, $\mathcal{P}(\cdot | s,a)$ is the distribution of the next state $s'$ given that action $a$ is taken in state $s$, $\mathcal{R}: \mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is the reward function, $p_0$ is the initial-state distribution, and $\gamma\in [0,1)$ is the discount factor. We assume the reward function to be uniformly bounded by a constant $R_{max}>0$. A deterministic policy $\pi : \mathcal{S} \rightarrow \mathcal{A}$ is a mapping from states to actions. At the beginning of each episode of interaction, the initial state $s_0$ is drawn from $p_0$. Then, the agent takes the action $a_0 = \pi(s_0)$, receives a reward $\mathcal{R}(s_0,a_0)$, transitions to the next state $s_1 \sim \mathcal{P}(\cdot | s_0,a_0)$, and the process is repeated. The goal is to find the policy maximizing the long-term return over a possibly infinite horizon: $\max_{\pi}J(\pi)=\mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r_t \mid \mathcal{M},\pi]$. To this end, we define the optimal value function $Q^*(s,a)$ as the expected return obtained by taking action $a$ in state $s$ and following an optimal policy thereafter. Then, an optimal policy $\pi^*$ is a policy that is greedy with respect to the optimal value function, i.e., $\pi^*(s) = \argmax_a Q^*(s,a)$ for all states $s$. It can be shown (e.g., \cite{puterman1994markov}) that $Q^*$ is the unique fixed-point of the optimal Bellman operator $T$ defined by $TQ(s,a) = \mathcal{R}(s,a) + \gamma\mathbb{E}_{\mathcal{P}}[\max_{a'}Q(s',a')]$ for any value function $Q$. From now on, we adopt the term $Q$-function to denote any plausible value function, i.e., any function $Q : \mathcal{S}\times\mathcal{A} \rightarrow \mathbb{R}$ uniformly bounded by $\frac{R_{max}}{1-\gamma}$.

We define the Bellman residual of a $Q$-function $Q$ as $B(Q) \triangleq TQ - Q$. Notice that a $Q$-function $Q$ is optimal if, and only if, $B(Q)(s,a) = 0$ for all $s,a$.

\paragraph{Multitask Settings}

We represent tasks $\tau$ as MDPs with shared state and action spaces, but with potentially different values for all other parameters. We assume the existence of a distribution $\mathcal{D}$ over tasks, i.e., $\tau \sim \mathcal{D}$, and we suppose that we are able to sample from such distribution.

\section{Approach}

We start by noticing that the task distribution $\mathcal{D}$ clearly induces a distribution over optimal $Q$-functions. Then, our goal is to estimate such distribution from the set of source tasks and use it as a prior for speeding up the learning process in the target task.

Consider solving the target task given a dataset $D$ of $N$ samples. The posterior distribution over optimal $Q$-functions is:
\begin{equation}
P(Q \mid D) \propto P(D \mid Q)P(Q),
\end{equation}
where $P(D \mid Q)$ is the likelihood of observing the dataset $D$ given that $Q$ is optimal and $P(Q)$ is the prior distribution induced by $\mathcal{D}$. Computing the likelihood requires knowing the transition and reward models of the current task (i.e., the MDP model), which are not available in practice. However, since we are conditioned on the fact that $Q$ is optimal, we can derive a simple approximation. We consider the empirical Bellman error $||B(Q)||_{p,D}^p$ under the $l_p$-norm defined as:
\begin{equation}
||B(Q)||_{p,D}^p = \frac{1}{N}\sum_{i=1}^N \left\lvert r_i + \gamma\max_{a'}Q(s_i',a') - Q(s_i,a_i) \right\rvert^p
\end{equation}
Assuming states and actions to be drawn from a fixed joint distribution $\mu$, we have $||B(Q)||_{p,\mu}^p = 0$ whenever $Q$ is optimal. Then, the probability of observing a dataset $D$ clearly decreases exponentially as $||B(Q)||_{p,D}^p$, for  an optimal $Q$, increases. This can be seen, for instance, by applying Hoeffding's inequality. Thus, a natural way to model the likelihood $P(D \mid Q)$ is:
\begin{equation}
P(D \mid Q) \propto e^{-\lambda N ||B(Q)||_{p,D}^p}
\end{equation}
where $\lambda$ is a constant hyperparameter. Intuitively, this indicates that $D$ is more likely when it induces low empirical Bellman error under an optimal $Q$-function. Furthermore, as the number of available samples $N$ increases, the distribution becomes more peaked at zero since the empirical Bellman error converges to the true Bellman error. In the limit $N \rightarrow \infty$:
\begin{equation}
P(D \mid Q) \propto I_{\left\{ ||B(Q)||_{p,D}^p = 0 \right\}}
\end{equation}

\subsection{Regularized Bellman Residual Minimization with Gaussian Priors}

Taking the maximum of the log-posterior, we obtain the following optimization problem:
\begin{equation}
\min_{Q\in\mathcal{Q}} ||B(Q)||_{p,D}^p - \log P(Q) 
\end{equation}
Let us specify a particular hypothesis space $\mathcal{Q}$. We consider $Q$-functions $Q_{\bm{w}}$ parameterized by the vector $\bm{w}$. Our optimization problem becomes:
\begin{equation}
\min_{\bm{w}} ||B(\bm{w})||_{p,D}^p - \log P(\bm{w}) 
\end{equation}
We model the prior distribution over the optimal parameters $\bm{w}$ as a Gaussian. That is, we assume:
\begin{equation}
P(\bm{w}) = \mathcal{N}(\bm{\mu},\bm{\Sigma})
\end{equation}
Then, our optimization, adopting the $l_2$-norm, becomes:
\begin{equation} \label{eq:optim}
\min_{\bm{w}} ||B(\bm{w})||_{D}^2 + ||\bm{w}-\bm{\mu}||_{\bm{\Sigma}} = \min_{\bm{w}} \frac{1}{N}\sum_{i=1}^N\left\lvert y_i - Q_{\bm{w}}(s_i,a_i) \right\rvert^2 + ||\bm{w}-\bm{\mu}||_{\bm{\Sigma}}
\end{equation}
where $y_i = r_i + \gamma\max_{a'}Q_{\bm{w}}(s_i',a')$ and $||\bm{x}||_{\bm{A}} \triangleq \bm{x}^T\bm{A}^{-1}\bm{x}$ for $\bm{A}$ positive definite matrix.

\paragraph{Linear model}

We assume a linear model for the $Q$-functions: $Q_{\bm{w}}(s,a) = \bm{w}^T\bm{\phi}(s,a)$. Here $\bm{\phi}$ is a $K$-dimensional feature vector. Then, the solution to the optimization problem of Eq. \eqref{eq:optim} can be computed in closed form as follows:
\begin{equation}
\bm{w}^* = \left( \bm{A}^T\bm{A} + \bm{\Sigma}^{-1} \right)^{-1} \left( \bm{A}^T\bm{b} + \bm{\Sigma}^{-1}\bm{\mu} \right) 
\end{equation}
where $\bm{A}$ is an $N\times K$ matrix containing the feature vectors at each data point $(s_i,a_i)$ and $\bm{b}$ is an $N$-dimensional vector containing their targets $y_i$. The resulting algorithm is a regularized version of LSVI.

\paragraph{Neural network}

We model $Q_{\bm{w}}(s,a)$ as a neural network with parameters $\bm{w}$. Then, the gradient of the objective function of Eq. \eqref{eq:optim} can be easily computed by standard backpropagation. The resulting algorithm is a regularized version of NFQI.

\subsection{Variational Inference for Gaussian Distributions}

TODO

\section{Related Works}

List of any paper that relates to our approach together with a brief description:
\begin{itemize}
\item \cite{osband2014generalization}: the authors propose a method for efficient exploration via randomized value functions. The optimal $Q$-function is computed by bayesian LSVI and, after each update, parameters are sampled from the posterior. Then, the agent follows a greedy policy with respect to the sampled $Q$-function. Regret bounds are provided.
\item \cite{osband2016deep}: the authors extend the idea of randomized value functions to drive exploration in deep RL. A posterior distribution over $Q$-functions is approximated via bootstrapping. In each episode, the agent acts greedily with respect to a $Q$-function sampled from the approximated posterior.
\item \cite{chen2017ucb}: the authors build on top of bootstrapped DQNs to provide UCB-like exploration bonuses. In a previous (?) version of the paper, exploration bonuses based on information gain are also proposed.
\end{itemize}

\section{Experiments}

\section{Conclusion}

{\small 
\bibliography{biblio.bib}
\bibliographystyle{plain}
}
\newpage
\appendix

\section{Proofs}

\end{document}
